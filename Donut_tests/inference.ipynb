{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from warnings import filterwarnings\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your checkpoint directory\n",
    "checkpoint_path = \"/home/arjun/Desktop/Github/AmazonML-Hackathon/Donut_tests/donut-finetuned-docvqa/checkpoint-5500\"\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "# Load the processor from the checkpoint\n",
    "# processor = DonutProcessor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Determine the device to use\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model from the checkpoint and move it to the appropriate device\n",
    "model = VisionEncoderDecoderModel.from_pretrained(checkpoint_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "def perform_inference(image, question):\n",
    "    # Prepare the input\n",
    "    encoding = processor(images=image, text=question, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move input to the correct device\n",
    "    for k, v in encoding.items():\n",
    "        encoding[k] = v.to(device)\n",
    "    print('start giving outputs')\n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        pixel_values=encoding[\"pixel_values\"],\n",
    "        # decoder_input_ids=encoding[\"decoder_input_ids\"],\n",
    "        max_length=model.config.max_length,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = sequence.strip()\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/home/arjun/Desktop/Github/AmazonML-Hackathon/dataset/test.csv')\n",
    "\n",
    "# Define the entity_unit_map (as in your training script)\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_image_path(image_link, image_folder):\n",
    "    # Extract filename from the URL\n",
    "    filename = image_link.split('/')[-1]\n",
    "    # Construct the full path to the local image file\n",
    "    local_path = os.path.join(image_folder, filename)\n",
    "    return local_path\n",
    "\n",
    "image_folder = '/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test_images'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    image_path = get_local_image_path(row['image_link'], image_folder)\n",
    "    \n",
    "    if row['entity_name'] in entity_unit_map:\n",
    "        potential_units = entity_unit_map[row['entity_name']]\n",
    "        question = f\"Given the image, what is the {row['entity_name']} of the item in {potential_units}: \"\n",
    "    else:\n",
    "        question = f\"Given the image, what is the {row['entity_name']}: \"\n",
    "    \n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    prediction = perform_inference(image_path, question)\n",
    "    results.append({'index': row['index'], 'prediction': prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_df.to_csv('inference_results.csv', index=False)\n",
    "\n",
    "print(\"Inference completed. Results saved to 'inference_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/131187 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=500x500 at 0x77265D250970> Given the image, what is the height of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/131187 [00:01<58:04:12,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1000x65 at 0x77265D24FDF0> Given the image, what is the width of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/131187 [00:03<57:34:53,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1000x65 at 0x77265C54B880> Given the image, what is the height of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/131187 [00:04<57:00:17,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1000x65 at 0x77265C54B9A0> Given the image, what is the depth of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/131187 [00:06<57:00:52,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1000x65 at 0x77265C561240> Given the image, what is the depth of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/131187 [00:07<55:57:03,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1000x65 at 0x77265C561120> Given the image, what is the height of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/131187 [00:09<57:19:55,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1000x65 at 0x77265C561090> Given the image, what is the width of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n",
      "start giving outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/131187 [00:11<57:39:55,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=500x500 at 0x77265C5A5720> Given the image, what is the height of the item in {'yard', 'metre', 'foot', 'millimetre', 'inch', 'centimetre'}: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven the image, what is the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(image, question)\n\u001b[0;32m---> 11\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mperform_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m: prediction})\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mperform_inference\u001b[0;34m(image, question)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_inference\u001b[39m(image, question):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Prepare the input\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Move input to the correct device\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encoding\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/models/donut/processing_donut.py:87\u001b[0m, in \u001b[0;36mDonutProcessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `images` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/utils/generic.py:852\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/models/donut/image_processing_donut.py:443\u001b[0m, in \u001b[0;36mDonutImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_thumbnail, do_align_long_axis, do_pad, random_padding, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    435\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_image(\n\u001b[1;32m    437\u001b[0m             image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, random_padding\u001b[38;5;241m=\u001b[39mrandom_padding, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format\n\u001b[1;32m    438\u001b[0m         )\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    440\u001b[0m     ]\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[0;32m--> 443\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    446\u001b[0m     ]\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[1;32m    449\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image\u001b[38;5;241m=\u001b[39mimage, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    452\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/models/donut/image_processing_donut.py:444\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    435\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_image(\n\u001b[1;32m    437\u001b[0m             image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, random_padding\u001b[38;5;241m=\u001b[39mrandom_padding, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format\n\u001b[1;32m    438\u001b[0m         )\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    440\u001b[0m     ]\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    443\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrescale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    446\u001b[0m     ]\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[1;32m    449\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image\u001b[38;5;241m=\u001b[39mimage, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    452\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/image_processing_utils.py:76\u001b[0m, in \u001b[0;36mBaseImageProcessor.rescale\u001b[0;34m(self, image, scale, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrescale\u001b[39m(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     48\u001b[0m     image: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Rescale an image by a scale factor. image = image * scale.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m        `np.ndarray`: The rescaled image.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrescale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/image_transforms.py:130\u001b[0m, in \u001b[0;36mrescale\u001b[0;34m(image, scale, data_format, dtype, input_data_format)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     rescaled_image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(rescaled_image, data_format, input_data_format)\n\u001b[0;32m--> 130\u001b[0m rescaled_image \u001b[38;5;241m=\u001b[39m \u001b[43mrescaled_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rescaled_image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# results = []\n",
    "# for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "#     image_link = download_image(row['image_link'])\n",
    "#     img_files = os.listdir('/home/arjun/Desktop/Github/AmazonML-Hackathon/images/test_images')\n",
    "#     if row['entity_name'] in entity_unit_map:\n",
    "#         potential_units = entity_unit_map[row['entity_name']]\n",
    "#         question = f\"Given the image, what is the {row['entity_name']} of the item in {potential_units}: \"\n",
    "#     else:\n",
    "#         question = f\"Given the image, what is the {row['entity_name']}: \"\n",
    "#     print(image, question)\n",
    "#     prediction = perform_inference(image, question)\n",
    "#     results.append({'index': row['index'], 'prediction': prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0, 'prediction': '<s><s> 100.0 gram'},\n",
       " {'index': 1, 'prediction': '<s><s> 100.0 gram'},\n",
       " {'index': 2, 'prediction': '<s><s> 100.0 gram'},\n",
       " {'index': 3, 'prediction': '<s><s> 100.0 gram'},\n",
       " {'index': 4, 'prediction': '<s><s> 100.0 gram'},\n",
       " {'index': 5, 'prediction': '<s><s> 100.0 gram'},\n",
       " {'index': 6, 'prediction': '<s><s> 100.0 gram'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
