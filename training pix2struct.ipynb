{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweringDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict], processor: Pix2StructProcessor, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.valid_samples = []\n",
    "        self._filter_valid_samples()\n",
    "\n",
    "    def _filter_valid_samples(self):\n",
    "        for idx, sample in enumerate(self.data):\n",
    "            try:\n",
    "                image_path = sample.get('image_path')\n",
    "                question = sample.get('question')\n",
    "                answer = sample.get('answer')\n",
    "\n",
    "                if not image_path or not os.path.exists(image_path):\n",
    "                    logger.warning(f\"Image file not found or invalid path: {image_path}\")\n",
    "                    continue\n",
    "\n",
    "                if not question or not answer:\n",
    "                    logger.warning(f\"Missing question or answer for sample {idx}\")\n",
    "                    continue\n",
    "\n",
    "                # Try opening the image to check if it's valid\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.verify()  # Verify that it's a valid image file\n",
    "\n",
    "                self.valid_samples.append(idx)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing sample {idx}: {str(e)}\")\n",
    "\n",
    "        logger.info(f\"Total samples: {len(self.data)}, Valid samples: {len(self.valid_samples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[self.valid_samples[idx]]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(sample['image_path']).convert('RGB')\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = self.processor(\n",
    "                images=image,\n",
    "                text=sample['question'],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Prepare the labels (answers)\n",
    "            labels = self.processor(\n",
    "                text=sample['answer'],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True\n",
    "            ).input_ids\n",
    "\n",
    "            # Remove batch dimension\n",
    "            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "            inputs['labels'] = labels.squeeze(0)\n",
    "\n",
    "            return inputs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing sample {idx} during __getitem__: {str(e)}\")\n",
    "            # Return a default or \"empty\" sample\n",
    "            return {k: torch.tensor([]) for k in ['input_ids', 'attention_mask', 'pixel_values', 'labels']}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'pixel_values': pixel_values\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred, processor):\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    tokenizer = processor.tokenizer\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Ensure all values are floats for JSON serialization\n",
    "    return {key: float(value * 100) for key, value in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\71gSRbyXmoL.jpg\", \"question\": \"What is the item volume?\", \"answer\": \"1.0 cup\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61BZ4zrjZXL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"0.709 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61I9XdN6OFL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"500.0 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\612mrlqiI4L.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"0.709 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\617Tl40LOXL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1400 milligram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61QsBSE7jgL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1400 milligram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81xsq6vf2qL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1400 milligram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\71DiLRHeZdL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1400 milligram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\91Cma3RzseL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1400 milligram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\71jBLhmTNlL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1400 milligram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81N73b5khVL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"30.0 kilogram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61oMj2iXOuL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"10 kilogram to 15 kilogram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\91LPf6OjV9L.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"3.53 ounce\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81fOxWWWKYL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"3.53 ounce\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81dzao1Ob4L.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"53 ounce\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\91-iahVGEDL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"100 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81S2+GnYpTL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"200 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81e2YtCOKvL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"1 kilogram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81RNsNEM1EL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"200 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\91prZeizZnL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"200 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\31EvJszFVfL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"200 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61wzlucTREL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"4.0 gallon\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61sQ+qAKr4L.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"2.7 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\81x77l2T5NL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"112 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\71nywfWZUwL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"4.1 kilogram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\71nywfWZUwL.jpg\", \"question\": \"What is the voltage?\", \"answer\": \"48.0 volt\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\51WsuKKAVrL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"158.0 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\61XGDKap+JL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"158.0 gram\"},\n",
    "    {\"image_path\": r\"C:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\images\\train\\715vVcWJxGL.jpg\", \"question\": \"What is the item weight?\", \"answer\": \"5000 milligram\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All image files are present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "missing_files = []\n",
    "\n",
    "for item in data:\n",
    "    image_path = item['image_path']\n",
    "    if not os.path.exists(image_path):\n",
    "        missing_files.append(image_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"The following image files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"All image files are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-docvqa-base\")\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-docvqa-base\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total samples: 29, Valid samples: 29\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "full_dataset = QuestionAnsweringDataset(data=data, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=partial(compute_metrics, processor=processor),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f668442d277e4a6ab7f92b7236c919db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error processing sample 18 during __getitem__: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'NoneType'>.\n",
      "ERROR:__main__:Error processing sample 15 during __getitem__: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'NoneType'>.\n",
      "ERROR:__main__:Error processing sample 7 during __getitem__: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'NoneType'>.\n",
      "ERROR:__main__:Error processing sample 1 during __getitem__: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'NoneType'>.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\transformers\\trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2233\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2237\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\accelerate\\data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\AmazonML-Hackathon\\venv\\lib\\site-packages\\transformers\\trainer_utils.py:814\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[0;32m    813\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 73\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[1;32m---> 73\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m     74\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m     75\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "Cell \u001b[1;32mIn[34], line 73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[1;32m---> 73\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m     74\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m     75\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
